---
skill_id: data-analysis-data-validation
skill_name: PPC æ•°æ®è´¨é‡éªŒè¯æŠ€èƒ½
category: data-analysis
version: 1.0.0
created: 2025-11-05
updated: 2025-11-05
status: active

# ç›¸å…³çš„æœ€ä½³å®è·µ
best_practices:
  - best-practices/data-analysis/data-quality-checklist

# ç›¸å…³çš„å­¦ä¹ è®°å½•
learnings:
  - learnings/2025-11/2025-11-05-ppc-data-quality

# ä¾èµ–çš„å…¶ä»– Skills
dependencies: []

# è¾“å…¥è¾“å‡ºå®šä¹‰
inputs:
  - data_files: CSV/Excel  # PPC æ•°æ®æ–‡ä»¶åˆ—è¡¨
  - expected_date_range: Object  # é¢„æœŸçš„æ—¥æœŸèŒƒå›´ {start, end}
outputs:
  - validation_report: Markdown  # æ•°æ®éªŒè¯æŠ¥å‘Š
  - cleaned_data: DataFrame  # æ¸…æ´—åçš„æ•°æ®
  - quality_score: Number  # æ•°æ®è´¨é‡è¯„åˆ† (0-100)

# æ ‡ç­¾
tags:
  - data-validation
  - data-quality
  - ppc-analysis
  - preprocessing
---

# PPC æ•°æ®è´¨é‡éªŒè¯æŠ€èƒ½

**åœ¨è¿›è¡Œä»»ä½• PPC åˆ†æä¹‹å‰,ç³»ç»ŸåŒ–åœ°éªŒè¯æ•°æ®å®Œæ•´æ€§ã€å‡†ç¡®æ€§å’Œä¸€è‡´æ€§çš„å…³é”®æŠ€èƒ½ã€‚**

---

## ğŸ“ æŠ€èƒ½æè¿°

è¿™ä¸ªæŠ€èƒ½æä¾›äº†ä¸€å¥—å®Œæ•´çš„ PPC æ•°æ®éªŒè¯æµç¨‹,ç¡®ä¿åˆ†æåŸºäºé«˜è´¨é‡çš„æ•°æ®ã€‚éªŒè¯èŒƒå›´åŒ…æ‹¬:
- æ—¥æœŸèŒƒå›´ä¸€è‡´æ€§æ£€æŸ¥
- å¿…éœ€å­—æ®µå®Œæ•´æ€§éªŒè¯
- æ•°æ®é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥ (clicks â‰¤ impressions, orders â‰¤ clicks)
- å¼‚å¸¸å€¼è¯†åˆ«å’Œå¤„ç†
- é›¶æ•°æ®å’Œç©ºå€¼æ£€æµ‹
- æ•°æ®ç±»å‹éªŒè¯

**æ ¸å¿ƒä»·å€¼:** é˜²æ­¢åŸºäºé”™è¯¯æˆ–ä¸å®Œæ•´æ•°æ®åšå‡ºé”™è¯¯çš„ä¼˜åŒ–å†³ç­–,è¿™å¯èƒ½å¯¼è‡´å¹¿å‘Šé¢„ç®—æµªè´¹ã€‚

---

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### é€‚ç”¨æƒ…å†µ

- **å®šæœŸåˆ†æå‰** - æ¯æ¬¡è¿›è¡Œ PPC åˆ†æä¹‹å‰çš„å¿…éœ€æ­¥éª¤
- **å¤šæ•°æ®æºæ•´åˆ** - åˆå¹¶ AI å’Œ Manual å¹¿å‘Šæ•°æ®æ—¶
- **æ•°æ®å¯¼å‡ºå** - ä»äºšé©¬é€Šå¹¿å‘Šå¹³å°å¯¼å‡ºæ•°æ®åç«‹å³éªŒè¯
- **å¼‚å¸¸å‘ç°** - å‘ç°æŒ‡æ ‡å¼‚å¸¸æ—¶,é¦–å…ˆéªŒè¯æ•°æ®è´¨é‡
- **è‡ªåŠ¨åŒ–æµç¨‹** - é›†æˆåˆ°è‡ªåŠ¨åŒ–åˆ†ææµç¨‹çš„ç¬¬ä¸€æ­¥

### ä¸é€‚ç”¨æƒ…å†µ

- **å®æ—¶ç›‘æ§** - å®æ—¶æ•°æ®æµä¸éœ€è¦å®Œæ•´éªŒè¯
- **å•ä¸€æŒ‡æ ‡æŸ¥è¯¢** - åªæŸ¥çœ‹æŸä¸€ä¸ªç®€å•æŒ‡æ ‡æ—¶
- **æ ·æœ¬æ•°æ®æ¢ç´¢** - åˆæ­¥æ•°æ®æ¢ç´¢é˜¶æ®µ

---

## ğŸ”§ å®ç°æ­¥éª¤

### æ­¥éª¤ 1: æ•°æ®åŠ è½½å’Œåˆæ­¥æ£€æŸ¥

**ç›®çš„:** æˆåŠŸåŠ è½½æ•°æ®å¹¶è¿›è¡ŒåŸºç¡€ç»“æ„æ£€æŸ¥

**æ“ä½œ:**
```python
import pandas as pd
import numpy as np
from datetime import datetime

def load_and_inspect_data(filepath):
    """åŠ è½½æ•°æ®å¹¶è¿›è¡Œåˆæ­¥æ£€æŸ¥"""
    # æ”¯æŒ CSV å’Œ Excel æ ¼å¼
    if filepath.endswith('.csv'):
        df = pd.read_csv(filepath, encoding='utf-8-sig')
    else:
        df = pd.read_excel(filepath)

    # æ¸…ç†åˆ—å (å»é™¤ç©ºæ ¼å’Œå¼•å·)
    df.columns = df.columns.str.strip().str.replace('"', '')

    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
    print(f"   - æ€»è¡Œæ•°: {len(df):,}")
    print(f"   - æ€»åˆ—æ•°: {len(df.columns)}")
    print(f"   - å†…å­˜å ç”¨: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

    return df

# ä½¿ç”¨ç¤ºä¾‹
ai_data = load_and_inspect_data('ai_summary.csv')
manual_data = load_and_inspect_data('manual_summary.csv')
```

**è¾“å‡º:**
- åŠ è½½çš„ DataFrame
- åŸºç¡€ç»Ÿè®¡ä¿¡æ¯

**æ³¨æ„äº‹é¡¹:**
- âš ï¸ æ³¨æ„ä¸­æ–‡ç¼–ç é—®é¢˜,ä½¿ç”¨ `encoding='utf-8-sig'`
- âš ï¸ å¤§æ–‡ä»¶ (>100MB) è€ƒè™‘ä½¿ç”¨ chunked reading
- âš ï¸ æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®

---

### æ­¥éª¤ 2: æ—¥æœŸèŒƒå›´ä¸€è‡´æ€§éªŒè¯

**ç›®çš„:** ç¡®ä¿å¯¹æ¯”æ•°æ®çš„æ—¥æœŸèŒƒå›´å®Œå…¨ä¸€è‡´

**æ“ä½œ:**
```python
def validate_date_ranges(df1, df2, date_column='æ—¥æœŸ'):
    """éªŒè¯ä¸¤ä¸ªæ•°æ®é›†çš„æ—¥æœŸèŒƒå›´æ˜¯å¦ä¸€è‡´"""
    # è½¬æ¢æ—¥æœŸåˆ—
    df1[date_column] = pd.to_datetime(df1[date_column])
    df2[date_column] = pd.to_datetime(df2[date_column])

    # æå–æ—¥æœŸèŒƒå›´
    df1_start = df1[date_column].min()
    df1_end = df1[date_column].max()
    df1_days = len(df1[date_column].unique())

    df2_start = df2[date_column].min()
    df2_end = df2[date_column].max()
    df2_days = len(df2[date_column].unique())

    # æ£€æŸ¥ä¸€è‡´æ€§
    is_consistent = (
        df1_start == df2_start and
        df1_end == df2_end and
        df1_days == df2_days
    )

    report = {
        'dataset1': {
            'start_date': df1_start,
            'end_date': df1_end,
            'unique_days': df1_days
        },
        'dataset2': {
            'start_date': df2_start,
            'end_date': df2_end,
            'unique_days': df2_days
        },
        'is_consistent': is_consistent
    }

    if is_consistent:
        print(f"âœ… æ—¥æœŸèŒƒå›´ä¸€è‡´")
        print(f"   - èµ·å§‹æ—¥æœŸ: {df1_start.date()}")
        print(f"   - ç»“æŸæ—¥æœŸ: {df1_end.date()}")
        print(f"   - å¤©æ•°: {df1_days} å¤©")
    else:
        print(f"âš ï¸  æ—¥æœŸèŒƒå›´ä¸ä¸€è‡´!")
        print(f"   Dataset 1: {df1_start.date()} åˆ° {df1_end.date()} ({df1_days} å¤©)")
        print(f"   Dataset 2: {df2_start.date()} åˆ° {df2_end.date()} ({df2_days} å¤©)")

    return report

# ä½¿ç”¨ç¤ºä¾‹
date_validation = validate_date_ranges(ai_data, manual_data)
```

**è¾“å‡º:**
- æ—¥æœŸèŒƒå›´å¯¹æ¯”æŠ¥å‘Š
- ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ

**æ³¨æ„äº‹é¡¹:**
- âš ï¸ å¦‚æœæ—¥æœŸèŒƒå›´ä¸ä¸€è‡´,å¿…é¡»è¿‡æ»¤åˆ°ç›¸åŒèŒƒå›´æˆ–æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
- âš ï¸ æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±çš„æ—¥æœŸ (gaps)
- âš ï¸ æ³¨æ„æ—¶åŒºé—®é¢˜

---

### æ­¥éª¤ 3: å¿…éœ€å­—æ®µå®Œæ•´æ€§éªŒè¯

**ç›®çš„:** ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„å­—æ®µéƒ½å­˜åœ¨ä¸”æœ‰æ•°æ®

**æ“ä½œ:**
```python
def validate_required_fields(df, required_fields):
    """éªŒè¯å¿…éœ€å­—æ®µçš„å­˜åœ¨æ€§å’Œå®Œæ•´æ€§"""
    validation_results = {
        'missing_columns': [],
        'empty_columns': [],
        'partially_empty': {},
        'all_valid': True
    }

    for field in required_fields:
        # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
        if field not in df.columns:
            validation_results['missing_columns'].append(field)
            validation_results['all_valid'] = False
            continue

        # æ£€æŸ¥ç©ºå€¼æ¯”ä¾‹
        null_count = df[field].isna().sum()
        null_percentage = (null_count / len(df)) * 100

        if null_count == len(df):
            # å®Œå…¨ä¸ºç©º
            validation_results['empty_columns'].append(field)
            validation_results['all_valid'] = False
        elif null_count > 0:
            # éƒ¨åˆ†ä¸ºç©º
            validation_results['partially_empty'][field] = {
                'null_count': null_count,
                'null_percentage': round(null_percentage, 2)
            }
            if null_percentage > 10:  # è¶…è¿‡10%ç©ºå€¼è§†ä¸ºæœ‰é—®é¢˜
                validation_results['all_valid'] = False

    # æ‰“å°æŠ¥å‘Š
    if validation_results['all_valid']:
        print(f"âœ… æ‰€æœ‰å¿…éœ€å­—æ®µéªŒè¯é€šè¿‡")
    else:
        if validation_results['missing_columns']:
            print(f"âŒ ç¼ºå¤±å­—æ®µ: {', '.join(validation_results['missing_columns'])}")
        if validation_results['empty_columns']:
            print(f"âŒ ç©ºå­—æ®µ: {', '.join(validation_results['empty_columns'])}")
        if validation_results['partially_empty']:
            print(f"âš ï¸  éƒ¨åˆ†ç©ºå€¼å­—æ®µ:")
            for field, stats in validation_results['partially_empty'].items():
                print(f"   - {field}: {stats['null_count']} ç©ºå€¼ ({stats['null_percentage']}%)")

    return validation_results

# PPC æ•°æ®å¿…éœ€å­—æ®µ
REQUIRED_FIELDS = [
    'æ—¥æœŸ', 'æ›å…‰é‡', 'ç‚¹å‡»', 'èŠ±è´¹', 'é”€å”®é¢',
    'å¹¿å‘Šè®¢å•', 'CPC', 'ACoS'
]

# ä½¿ç”¨ç¤ºä¾‹
field_validation = validate_required_fields(ai_data, REQUIRED_FIELDS)
```

**è¾“å‡º:**
- å­—æ®µå®Œæ•´æ€§æŠ¥å‘Š
- ç¼ºå¤±/ç©ºå€¼ç»Ÿè®¡

**æ³¨æ„äº‹é¡¹:**
- âš ï¸ ä¸åŒæŠ¥å‘Šç±»å‹çš„å¿…éœ€å­—æ®µå¯èƒ½ä¸åŒ (Summary vs Campaign vs Keyword)
- âš ï¸ æŸäº›å­—æ®µå…è®¸ä¸ºç©º (å¦‚æ–°å¹¿å‘Šæ´»åŠ¨å¯èƒ½æ²¡æœ‰è½¬åŒ–æ•°æ®)
- âœ… å¯¹éƒ¨åˆ†ç©ºå€¼å­—æ®µ,è®°å½•ä½†ä¸é˜»æ–­åˆ†æ

---

### æ­¥éª¤ 4: æ•°æ®é€»è¾‘ä¸€è‡´æ€§éªŒè¯

**ç›®çš„:** éªŒè¯æ•°æ®ä¹‹é—´çš„é€»è¾‘å…³ç³»æ˜¯å¦æ­£ç¡®

**æ“ä½œ:**
```python
def validate_data_logic(df):
    """éªŒè¯æ•°æ®çš„é€»è¾‘ä¸€è‡´æ€§"""
    errors = []
    warnings = []

    # è§„åˆ™ 1: Clicks â‰¤ Impressions
    if 'ç‚¹å‡»' in df.columns and 'æ›å…‰é‡' in df.columns:
        invalid_ctr = df[df['ç‚¹å‡»'] > df['æ›å…‰é‡']]
        if len(invalid_ctr) > 0:
            errors.append({
                'rule': 'Clicks â‰¤ Impressions',
                'violations': len(invalid_ctr),
                'rows': invalid_ctr.index.tolist()[:5]  # åªæ˜¾ç¤ºå‰5ä¸ª
            })

    # è§„åˆ™ 2: Orders â‰¤ Clicks
    if 'å¹¿å‘Šè®¢å•' in df.columns and 'ç‚¹å‡»' in df.columns:
        invalid_cvr = df[df['å¹¿å‘Šè®¢å•'] > df['ç‚¹å‡»']]
        if len(invalid_cvr) > 0:
            errors.append({
                'rule': 'Orders â‰¤ Clicks',
                'violations': len(invalid_cvr),
                'rows': invalid_cvr.index.tolist()[:5]
            })

    # è§„åˆ™ 3: Spend â‰¥ 0
    if 'èŠ±è´¹' in df.columns:
        negative_spend = df[df['èŠ±è´¹'] < 0]
        if len(negative_spend) > 0:
            errors.append({
                'rule': 'Spend â‰¥ 0',
                'violations': len(negative_spend),
                'rows': negative_spend.index.tolist()[:5]
            })

    # è§„åˆ™ 4: ACoS è®¡ç®—éªŒè¯ (å…è®¸å°è¯¯å·®)
    if all(col in df.columns for col in ['èŠ±è´¹', 'é”€å”®é¢', 'ACoS']):
        df_with_sales = df[df['é”€å”®é¢'] > 0].copy()
        df_with_sales['calculated_acos'] = (df_with_sales['èŠ±è´¹'] / df_with_sales['é”€å”®é¢'] * 100)

        # æ¸…ç† ACoS å€¼ (å¯èƒ½æ˜¯å­—ç¬¦ä¸²æ ¼å¼)
        def clean_acos(val):
            if pd.isna(val) or val == '' or val == '--':
                return 0
            if isinstance(val, str):
                val = val.replace('%', '').strip()
                if val == 'æœ‰èŠ±è´¹æ— é”€å”®é¢':
                    return 0
            return float(val)

        df_with_sales['ACoS'] = df_with_sales['ACoS'].apply(clean_acos)

        # æ£€æŸ¥å·®å¼‚ (å…è®¸ 5% è¯¯å·®)
        df_with_sales['acos_diff'] = abs(df_with_sales['ACoS'] - df_with_sales['calculated_acos'])
        significant_diff = df_with_sales[df_with_sales['acos_diff'] > 5]

        if len(significant_diff) > 0:
            warnings.append({
                'rule': 'ACoS Calculation Accuracy',
                'violations': len(significant_diff),
                'message': f'{len(significant_diff)} rows have ACoS calculation discrepancy > 5%'
            })

    # æ‰“å°ç»“æœ
    if len(errors) == 0 and len(warnings) == 0:
        print(f"âœ… æ•°æ®é€»è¾‘ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
    else:
        if errors:
            print(f"âŒ å‘ç° {len(errors)} ä¸ªé€»è¾‘é”™è¯¯:")
            for error in errors:
                print(f"   - {error['rule']}: {error['violations']} ä¸ªè¿è§„")
        if warnings:
            print(f"âš ï¸  å‘ç° {len(warnings)} ä¸ªè­¦å‘Š:")
            for warning in warnings:
                print(f"   - {warning['rule']}: {warning['message']}")

    return {'errors': errors, 'warnings': warnings}

# ä½¿ç”¨ç¤ºä¾‹
logic_validation = validate_data_logic(ai_data)
```

**è¾“å‡º:**
- é€»è¾‘é”™è¯¯åˆ—è¡¨
- è­¦å‘Šåˆ—è¡¨

**æ³¨æ„äº‹é¡¹:**
- âŒ å¦‚æœå‘ç°é€»è¾‘é”™è¯¯,åº”åœæ­¢åˆ†æå¹¶æŠ¥å‘Š
- âš ï¸ è­¦å‘Šå¯ä»¥ç»§ç»­åˆ†æ,ä½†éœ€è¦è®°å½•
- âœ… å¯¹äºè®¡ç®—å‹å­—æ®µ (å¦‚ ACoS),å…è®¸å°çš„å››èˆäº”å…¥è¯¯å·®

---

### æ­¥éª¤ 5: å¼‚å¸¸å€¼æ£€æµ‹

**ç›®çš„:** è¯†åˆ«ç»Ÿè®¡ä¸Šçš„å¼‚å¸¸å€¼,å¯èƒ½æ˜¯æ•°æ®é”™è¯¯æˆ–éœ€è¦ç‰¹åˆ«å…³æ³¨çš„æƒ…å†µ

**æ“ä½œ:**
```python
def detect_outliers(df, columns, method='iqr', threshold=3):
    """æ£€æµ‹å¼‚å¸¸å€¼"""
    outliers_report = {}

    for col in columns:
        if col not in df.columns:
            continue

        # ç¡®ä¿æ˜¯æ•°å€¼å‹
        df[col] = pd.to_numeric(df[col], errors='coerce')
        values = df[col].dropna()

        if len(values) == 0:
            continue

        if method == 'iqr':
            # IQR æ–¹æ³•
            Q1 = values.quantile(0.25)
            Q3 = values.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

        elif method == 'zscore':
            # Z-score æ–¹æ³•
            mean = values.mean()
            std = values.std()
            z_scores = np.abs((df[col] - mean) / std)
            outliers = df[z_scores > threshold]

        if len(outliers) > 0:
            outliers_report[col] = {
                'count': len(outliers),
                'percentage': round(len(outliers) / len(df) * 100, 2),
                'examples': outliers[col].head(5).tolist()
            }

    # æ‰“å°æŠ¥å‘Š
    if len(outliers_report) == 0:
        print(f"âœ… æœªæ£€æµ‹åˆ°ç»Ÿè®¡å¼‚å¸¸å€¼")
    else:
        print(f"âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸å€¼:")
        for col, stats in outliers_report.items():
            print(f"   - {col}: {stats['count']} ä¸ªå¼‚å¸¸å€¼ ({stats['percentage']}%)")
            print(f"     ç¤ºä¾‹: {stats['examples']}")

    return outliers_report

# ä½¿ç”¨ç¤ºä¾‹
NUMERIC_COLUMNS = ['æ›å…‰é‡', 'ç‚¹å‡»', 'èŠ±è´¹', 'é”€å”®é¢', 'CPC']
outliers = detect_outliers(ai_data, NUMERIC_COLUMNS, method='iqr')
```

**è¾“å‡º:**
- å¼‚å¸¸å€¼ç»Ÿè®¡æŠ¥å‘Š
- ç¤ºä¾‹å¼‚å¸¸å€¼

**æ³¨æ„äº‹é¡¹:**
- â„¹ï¸ å¼‚å¸¸å€¼ä¸ä¸€å®šæ˜¯é”™è¯¯,å¯èƒ½æ˜¯çœŸå®çš„ç‰¹æ®Šæƒ…å†µ (å¦‚å¤§ä¿ƒæœŸé—´)
- âš ï¸ éœ€è¦äººå·¥åˆ¤æ–­æ˜¯ä¿ç•™è¿˜æ˜¯æ’é™¤å¼‚å¸¸å€¼
- âœ… è®°å½•æ‰€æœ‰å¼‚å¸¸å€¼å¤„ç†å†³ç­–

---

### æ­¥éª¤ 6: ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š

**ç›®çš„:** ç”Ÿæˆå®Œæ•´çš„æ•°æ®éªŒè¯æŠ¥å‘Šå’Œè´¨é‡è¯„åˆ†

**æ“ä½œ:**
```python
def generate_quality_report(df, dataset_name,
                          date_validation,
                          field_validation,
                          logic_validation,
                          outliers_report):
    """ç”Ÿæˆå®Œæ•´çš„æ•°æ®è´¨é‡æŠ¥å‘Š"""

    # è®¡ç®—è´¨é‡è¯„åˆ† (0-100)
    score = 100

    # æ—¥æœŸä¸€è‡´æ€§ (20åˆ†)
    if not date_validation.get('is_consistent', True):
        score -= 20

    # å­—æ®µå®Œæ•´æ€§ (30åˆ†)
    if not field_validation.get('all_valid', True):
        missing_penalty = len(field_validation.get('missing_columns', [])) * 10
        empty_penalty = len(field_validation.get('empty_columns', [])) * 10
        partial_penalty = len(field_validation.get('partially_empty', {})) * 5
        score -= min(30, missing_penalty + empty_penalty + partial_penalty)

    # é€»è¾‘ä¸€è‡´æ€§ (30åˆ†)
    errors = logic_validation.get('errors', [])
    warnings = logic_validation.get('warnings', [])
    score -= len(errors) * 15
    score -= len(warnings) * 5
    score = max(0, score)  # ç¡®ä¿ä¸ä½äº0

    # å¼‚å¸¸å€¼ (20åˆ†) - è¾ƒå®½æ¾
    outlier_penalty = min(20, len(outliers_report) * 5)
    score -= outlier_penalty

    score = max(0, min(100, score))  # ç¡®ä¿åœ¨ 0-100 èŒƒå›´å†…

    # ç”ŸæˆæŠ¥å‘Š
    report = f"""
# æ•°æ®è´¨é‡éªŒè¯æŠ¥å‘Š

## æ•°æ®é›†: {dataset_name}

### æ€»ä½“è´¨é‡è¯„åˆ†: {score}/100

{"âœ… ä¼˜ç§€" if score >= 90 else "âš ï¸ è‰¯å¥½" if score >= 70 else "âŒ éœ€è¦æ”¹è¿›"}

---

## 1. æ—¥æœŸèŒƒå›´éªŒè¯

- èµ·å§‹æ—¥æœŸ: {date_validation.get('dataset1', {}).get('start_date', 'N/A')}
- ç»“æŸæ—¥æœŸ: {date_validation.get('dataset1', {}).get('end_date', 'N/A')}
- å¤©æ•°: {date_validation.get('dataset1', {}).get('unique_days', 'N/A')}
- çŠ¶æ€: {"âœ… é€šè¿‡" if date_validation.get('is_consistent', True) else "âŒ ä¸ä¸€è‡´"}

---

## 2. å­—æ®µå®Œæ•´æ€§éªŒè¯

- ç¼ºå¤±å­—æ®µ: {len(field_validation.get('missing_columns', []))} ä¸ª
- ç©ºå­—æ®µ: {len(field_validation.get('empty_columns', []))} ä¸ª
- éƒ¨åˆ†ç©ºå€¼å­—æ®µ: {len(field_validation.get('partially_empty', {}))} ä¸ª
- çŠ¶æ€: {"âœ… é€šè¿‡" if field_validation.get('all_valid', True) else "âŒ æœ‰é—®é¢˜"}

---

## 3. é€»è¾‘ä¸€è‡´æ€§éªŒè¯

- é€»è¾‘é”™è¯¯: {len(errors)} ä¸ª
- è­¦å‘Š: {len(warnings)} ä¸ª
- çŠ¶æ€: {"âœ… é€šè¿‡" if len(errors) == 0 else "âŒ æœ‰é”™è¯¯"}

---

## 4. å¼‚å¸¸å€¼æ£€æµ‹

- æ£€æµ‹å­—æ®µæ•°: {len(outliers_report)}
- çŠ¶æ€: {"âœ… æ— å¼‚å¸¸" if len(outliers_report) == 0 else "âš ï¸ å‘ç°å¼‚å¸¸å€¼"}

---

## å»ºè®®è¡ŒåŠ¨

"""

    if score >= 90:
        report += "- âœ… æ•°æ®è´¨é‡ä¼˜ç§€,å¯ä»¥ç›´æ¥è¿›è¡Œåˆ†æ\n"
    elif score >= 70:
        report += "- âš ï¸ æ•°æ®è´¨é‡è‰¯å¥½,å»ºè®®è§£å†³å‘ç°çš„é—®é¢˜åè¿›è¡Œåˆ†æ\n"
    else:
        report += "- âŒ æ•°æ®è´¨é‡è¾ƒå·®,å¼ºçƒˆå»ºè®®å…ˆæ¸…ç†æ•°æ®\n"

    if not date_validation.get('is_consistent', True):
        report += "- ğŸ”§ è¿‡æ»¤æ•°æ®åˆ°ç›¸åŒçš„æ—¥æœŸèŒƒå›´\n"

    if len(errors) > 0:
        report += "- ğŸ”§ ä¿®å¤é€»è¾‘é”™è¯¯åå†è¿›è¡Œåˆ†æ\n"

    print(report)

    return {
        'score': score,
        'report': report,
        'recommendation': 'proceed' if score >= 90 else 'fix_issues' if score >= 70 else 'clean_data'
    }

# ä½¿ç”¨ç¤ºä¾‹
quality_report = generate_quality_report(
    ai_data,
    "AI Campaign Data",
    date_validation,
    field_validation,
    logic_validation,
    outliers
)
```

**è¾“å‡º:**
- å®Œæ•´çš„ Markdown æ ¼å¼è´¨é‡æŠ¥å‘Š
- è´¨é‡è¯„åˆ† (0-100)
- è¡ŒåŠ¨å»ºè®®

---

## â­ æœ€ä½³å®è·µ

### å…³é”®åŸåˆ™

1. **éªŒè¯å…ˆäºåˆ†æ** - æ°¸è¿œä¸è¦è·³è¿‡æ•°æ®éªŒè¯æ­¥éª¤,å³ä½¿æ—¶é—´ç´§è¿«
2. **è‡ªåŠ¨åŒ–éªŒè¯** - å°†éªŒè¯æµç¨‹é›†æˆåˆ°è‡ªåŠ¨åŒ–è„šæœ¬ä¸­
3. **è®°å½•æ‰€æœ‰å†³ç­–** - è®°å½•ä¸ºä»€ä¹ˆä¿ç•™æˆ–æ’é™¤æŸäº›æ•°æ®
4. **åˆ†çº§ä¸¥é‡æ€§** - åŒºåˆ†é”™è¯¯ (é˜»æ–­åˆ†æ) å’Œè­¦å‘Š (è®°å½•ä½†ç»§ç»­)

### æ€§èƒ½ä¼˜åŒ–

- å¯¹å¤§æ•°æ®é›†ä½¿ç”¨é‡‡æ ·éªŒè¯ (éªŒè¯å‰1000è¡Œå’Œå1000è¡Œ)
- ç¼“å­˜éªŒè¯ç»“æœ,é¿å…é‡å¤éªŒè¯
- å¹¶è¡ŒéªŒè¯å¤šä¸ªæ•°æ®é›†

### è´¨é‡æ§åˆ¶

- âœ… è´¨é‡è¯„åˆ† â‰¥ 90 æ‰èƒ½ç›´æ¥ç”¨äºé‡è¦å†³ç­–
- âœ… æ‰€æœ‰é€»è¾‘é”™è¯¯å¿…é¡»ä¿®å¤
- âœ… ä¿å­˜éªŒè¯æŠ¥å‘Šåˆ°æ–‡ä»¶ç³»ç»Ÿ

---

## ğŸ“Š ç¤ºä¾‹

### ç¤ºä¾‹ 1: å®Œæ•´éªŒè¯æµç¨‹

**è¾“å…¥:**
```python
ai_summary_path = 'ai_summary.csv'
manual_summary_path = 'manual_summary.csv'
```

**æ‰§è¡Œ:**
```python
# æ­¥éª¤ 1: åŠ è½½æ•°æ®
ai_data = load_and_inspect_data(ai_summary_path)
manual_data = load_and_inspect_data(manual_summary_path)

# æ­¥éª¤ 2: æ—¥æœŸéªŒè¯
date_check = validate_date_ranges(ai_data, manual_data)

# æ­¥éª¤ 3: å­—æ®µéªŒè¯
field_check_ai = validate_required_fields(ai_data, REQUIRED_FIELDS)
field_check_manual = validate_required_fields(manual_data, REQUIRED_FIELDS)

# æ­¥éª¤ 4: é€»è¾‘éªŒè¯
logic_check_ai = validate_data_logic(ai_data)
logic_check_manual = validate_data_logic(manual_data)

# æ­¥éª¤ 5: å¼‚å¸¸å€¼æ£€æµ‹
outliers_ai = detect_outliers(ai_data, NUMERIC_COLUMNS)
outliers_manual = detect_outliers(manual_data, NUMERIC_COLUMNS)

# æ­¥éª¤ 6: ç”ŸæˆæŠ¥å‘Š
report_ai = generate_quality_report(
    ai_data, "AI Campaigns",
    date_check, field_check_ai, logic_check_ai, outliers_ai
)
report_manual = generate_quality_report(
    manual_data, "Manual Campaigns",
    date_check, field_check_manual, logic_check_manual, outliers_manual
)

# å†³ç­–
if report_ai['score'] >= 90 and report_manual['score'] >= 90:
    print("âœ… æ•°æ®éªŒè¯é€šè¿‡,å¼€å§‹åˆ†æ")
else:
    print("âš ï¸ æ•°æ®è´¨é‡é—®é¢˜,å»ºè®®å…ˆæ¸…ç†")
```

**è¾“å‡º:**
```
âœ… æ•°æ®åŠ è½½æˆåŠŸ
   - æ€»è¡Œæ•°: 31
   - æ€»åˆ—æ•°: 15
   - å†…å­˜å ç”¨: 0.12 MB

âœ… æ—¥æœŸèŒƒå›´ä¸€è‡´
   - èµ·å§‹æ—¥æœŸ: 2025-10-01
   - ç»“æŸæ—¥æœŸ: 2025-10-31
   - å¤©æ•°: 31 å¤©

âœ… æ‰€æœ‰å¿…éœ€å­—æ®µéªŒè¯é€šè¿‡
âœ… æ•°æ®é€»è¾‘ä¸€è‡´æ€§éªŒè¯é€šè¿‡
âœ… æœªæ£€æµ‹åˆ°ç»Ÿè®¡å¼‚å¸¸å€¼

æ•°æ®è´¨é‡è¯„åˆ†: 100/100 âœ… ä¼˜ç§€
å»ºè®®: å¯ä»¥ç›´æ¥è¿›è¡Œåˆ†æ
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### é—®é¢˜ 1: AI å’Œ Manual æ•°æ®æ—¥æœŸèŒƒå›´ä¸ä¸€è‡´

**ç—‡çŠ¶:** AI å¹¿å‘Šä»10æœˆ9æ—¥æ‰å¼€å§‹,ä½† Manual ä»10æœˆ1æ—¥å°±æœ‰æ•°æ®

**åŸå› :** AI å¹¿å‘Šæ˜¯åæœŸå¯åŠ¨çš„

**è§£å†³æ–¹æ¡ˆ:**
```python
# æ–¹æ¡ˆ 1: è¿‡æ»¤åˆ°ç›¸åŒæ—¥æœŸèŒƒå›´
common_start = max(ai_data['æ—¥æœŸ'].min(), manual_data['æ—¥æœŸ'].min())
common_end = min(ai_data['æ—¥æœŸ'].max(), manual_data['æ—¥æœŸ'].max())

ai_filtered = ai_data[
    (ai_data['æ—¥æœŸ'] >= common_start) &
    (ai_data['æ—¥æœŸ'] <= common_end)
]
manual_filtered = manual_data[
    (manual_data['æ—¥æœŸ'] >= common_start) &
    (manual_data['æ—¥æœŸ'] <= common_end)
]

# æ–¹æ¡ˆ 2: æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·å¹¶ä½¿ç”¨å…¨éƒ¨æ•°æ®
print(f"âš ï¸ æ³¨æ„: AI æ•°æ®èŒƒå›´ ({ai_data['æ—¥æœŸ'].min()} åˆ° {ai_data['æ—¥æœŸ'].max()}) "
      f"ä¸ Manual æ•°æ®èŒƒå›´ ({manual_data['æ—¥æœŸ'].min()} åˆ° {manual_data['æ—¥æœŸ'].max()}) ä¸ä¸€è‡´")
```

---

### é—®é¢˜ 2: Clicks > Impressions çš„é€»è¾‘é”™è¯¯

**ç—‡çŠ¶:** å‘ç°æŸäº›è¡Œçš„ç‚¹å‡»æ•°å¤§äºæ›å…‰æ•°

**åŸå› :** æ•°æ®å¯¼å‡ºé”™è¯¯æˆ–å¹³å°ç»Ÿè®¡bug

**è§£å†³æ–¹æ¡ˆ:**
```python
# è¯†åˆ«é—®é¢˜è¡Œ
invalid_rows = df[df['ç‚¹å‡»'] > df['æ›å…‰é‡']]

# æ–¹æ¡ˆ 1: æ’é™¤è¿™äº›è¡Œ
df_clean = df[df['ç‚¹å‡»'] <= df['æ›å…‰é‡']]
print(f"æ’é™¤äº† {len(invalid_rows)} ä¸ªé€»è¾‘é”™è¯¯è¡Œ")

# æ–¹æ¡ˆ 2: ä¿®æ­£ (å‡è®¾æ›å…‰é‡åº”è¯¥ç­‰äºç‚¹å‡»æ•°)
df.loc[df['ç‚¹å‡»'] > df['æ›å…‰é‡'], 'æ›å…‰é‡'] = df.loc[df['ç‚¹å‡»'] > df['æ›å…‰é‡'], 'ç‚¹å‡»']

# è®°å½•å†³ç­–
with open('data_cleaning_log.txt', 'a') as f:
    f.write(f"{datetime.now()}: ä¿®æ­£äº† {len(invalid_rows)} è¡Œçš„ Clicks > Impressions é—®é¢˜\n")
```

---

### é—®é¢˜ 3: å¤§é‡ç©ºå€¼æˆ–é›¶å€¼

**ç—‡çŠ¶:** æŸäº›å¤©çš„æ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯0

**åŸå› :** å¯èƒ½æ˜¯å¹¿å‘Šæš‚åœã€é¢„ç®—ç”¨å®Œã€æˆ–å‘¨æœ«æ— æ•°æ®

**è§£å†³æ–¹æ¡ˆ:**
```python
# è¯†åˆ«é›¶æ•°æ®æ—¥
zero_days = df[
    (df['æ›å…‰é‡'] == 0) &
    (df['ç‚¹å‡»'] == 0) &
    (df['èŠ±è´¹'] == 0)
]

print(f"å‘ç° {len(zero_days)} å¤©é›¶æ•°æ®:")
print(zero_days['æ—¥æœŸ'].tolist())

# å†³ç­–: æ˜¯å¦æ’é™¤é›¶æ•°æ®æ—¥
if len(zero_days) / len(df) < 0.1:  # å°‘äº10%
    # ä¿ç•™,ä½†åœ¨åˆ†æä¸­æ ‡æ³¨
    df['is_zero_day'] = (
        (df['æ›å…‰é‡'] == 0) &
        (df['ç‚¹å‡»'] == 0) &
        (df['èŠ±è´¹'] == 0)
    )
else:
    # æ’é™¤æˆ–è¿›ä¸€æ­¥è°ƒæŸ¥
    print("âš ï¸ é›¶æ•°æ®æ—¥è¶…è¿‡10%,å»ºè®®è°ƒæŸ¥åŸå› ")
```

---

## ğŸ”— ç›¸å…³èµ„æº

### ç›¸å…³ Skills

- [ppc-optimization](../amazon/advertising/ppc-optimization.md) - åœ¨åˆ†æå‰ä½¿ç”¨æ­¤ skill éªŒè¯æ•°æ®
- [data-cleaning](./data-cleaning.md) - æ•°æ®æ¸…æ´—æŠ€èƒ½
- [data-transformation](./data-transformation.md) - æ•°æ®è½¬æ¢æŠ€èƒ½

### ç›¸å…³ Agents

- [ad-analyzer](../../agents/ad-analyzer/agent.md) - ä½¿ç”¨æ­¤ skill éªŒè¯è¾“å…¥æ•°æ®
- [data-analyst](../../agents/data-analyst/agent.md) - ä¸“é—¨çš„æ•°æ®åˆ†æ agent

### å¤–éƒ¨æ–‡æ¡£

- [Pandas Data Validation](https://pandas.pydata.org/docs/) - Pandas å®˜æ–¹æ–‡æ¡£
- [Amazon Advertising API](https://advertising.amazon.com/API/docs) - äº†è§£æ•°æ®å­—æ®µå®šä¹‰

---

## ğŸ“ˆ æ”¹è¿›å†å²

### v1.0.0 (2025-11-05)

- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ ¸å¿ƒåŠŸèƒ½: 6æ­¥å®Œæ•´éªŒè¯æµç¨‹
- åŸºäºå®é™… PPC æ•°æ®éªŒè¯ç»éªŒæç‚¼
- åŒ…å«è´¨é‡è¯„åˆ†ç³»ç»Ÿ (0-100)
- æä¾›å®Œæ•´çš„ Python ä»£ç å®ç°

---

**ç»´æŠ¤è€…:** CharliephilMrlong
**æœ€åæ›´æ–°:** 2025-11-05
**çŠ¶æ€:** âœ… Active
**ä½¿ç”¨é¢‘ç‡:** Critical (æ¯æ¬¡ PPC åˆ†æçš„å¿…éœ€å‰ç½®æ­¥éª¤)
